<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kaiqu Liang</title>

    <meta name="author" content="Kaiqu Liang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kaiqu Liang
                </p>
                <p>I am a 3rd year PhD student in Computer Science at Princeton University, advised by <a href="https://saferobotics.princeton.edu/jaime">Jaime Fernández Fisac</a>. I also work closely with <a href="https://cocosci.princeton.edu/tom/tom.php">Tom Griffiths</a>.
                </p>
                <p>
                  Previously, I completed my MPhil in Machine Learning at the University of Cambridge, advised by <a href="https://samuelalbanie.com/">Samuel Albanie</a> and <a href="https://sites.google.com/view/bill-byrne/home/">Bill Byrne</a>. 
                   I did my undergraduate studies at the University of Toronto, where I was advised by <a href="https://www.cs.toronto.edu/~rgrosse/">Roger Grosse</a> and <a href="https://www.cs.toronto.edu/~sven/">Sven Dickinson</a>.
                   During my undergraduate years, I was also a student researcher in <a href="https://vectorinstitute.ai/">Vector Institute</a>, advised by Roger Grosse.
                </p>
                <p style="text-align:center">
                  <a href="mailto:kl2471@princeton.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=hmqvdJgAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/kaiqu_liang">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/kevinliang888">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/kaiqu-liang-00042b195/">Linkedin</a>
                </p>
              </td>
 
          <td style="padding:2.5%;width:40%;max-width:40%">
            <a href="images/photo.jpeg"><img style="width:80%;max-width:80%;object-fit: cover;" alt="profile photo" src="images/photo.jpeg" class="hoverZoomLink"></a>
        </td>
          </tbody></table>


          <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:top">
                  <h2 style="margin-bottom:20px;">News</h2> 
                  <div class="item">
                    <b>07/2025:</b>  Released <a href="https://arxiv.org/abs/2507.07484"> Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models </a> (<a href="https://machine-bullshit.github.io/">Website</a>, <a href="https://x.com/kaiqu_liang/status/1943350770788937980">Tweet</a>). &nbsp; Looking forward to future developments on understanding and mitigating Machine Bullshit!
                  </div>
                  <div class="item">
                    <b>06/2025:</b> Research Intern at Meta Superintelligence Labs.</a>
                  </div>
                  <div class="item">
                    <b>01/2025:</b> Released <a href="https://arxiv.org/abs/2501.08617"> RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation </a> (<a href="https://rl-hindsight.github.io/">Website</a>, <a href="https://x.com/kevin_lkq/status/1879909601291928009">Tweet</a>).
                  </div>
                  <div class="item"></div>
                    <b>12/2024:</b> Our workshop proposal - <a href="https://saferobotics.princeton.edu/ptas-icra25"> Public Trust in Autonomous Systems </a> - is accepted by <a href="https://2025.ieee-icra.org/"> ICRA 2025</a>! We look forward to hosting it in May 2025.
                  </div>
                  <div class="item">
                    <b>09/2024:</b> Our paper <a href="https://arxiv.org/abs/2402.06529">Introspective Planning</a> is accepted to NeurIPS 2024. 
                  </div>
                </td>
              </tr>
           </tbody>
          </table> 

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm working on human-AI safety, LLM planning & alignment, and embodied AI. My long-term goal is to build safe, reliable AI systems that effectively assist humans. Previously, I also did research on video retrieval and out-of-distribution generalization. <span style="color: #0074D9;"> I’m always open to collaboration. If you find our interests align, please feel free to drop me an email.</span><br><br>
                  - I've been considering these questions on human-AI safety & alignment:  
                  Are we underestimating the ways LLMs might subtly influence daily human decisions?
                  Do we recognize that certain harms might surface long after the interaction? 
                  How should our alignment algorithms account for these risks? <br><br>

                  - I've been considering these questions on embodied agents: Are LLMs capable enough for reliable planning? 
                  Do their actions truly align with human intentions? How can we ensure agents act safely, beneficially, and consistently for human needs?
 
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr class="featured-paper" onmouseout="intro_stop()" onmouseover="intro_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/teaser_bullshit.png" alt="intro" width="160" style="border-style: none">
              </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2507.07484">
                    <papertitle style="font-size: 100%;"><b>Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models</b></papertitle>
                  </a>
                  <br>
                  <span style="font-size:95%;">
                  <strong>Kaiqu Liang</strong>, Haimin Hu, Xuandong Zhao, Dawn Song, Thomas L. Griffiths, Jaime Fernández Fisac
                  </span>
                  <!-- <br>
                  <em> Preprint</em>
                  <br>    -->
                  <a href="https://arxiv.org/abs/2507.07484">Paper</a>     
                  |
                  <a href="https://machine-bullshit.github.io/">Website</a>
                  |
                  <a href="https://github.com/kevinliang888/Machine-Bullshit">Code</a>
                  |
                  <a href="https://huggingface.co/datasets/kaiquliang/BullshitEval">Data</a>
                  |
                  <a href="https://x.com/kaiqu_liang/status/1943350770788937980">Tweet</a>
                  <br>
                  <span style="font-size:100%;color:#555;">
                    <b>Media coverage:</b>
                    <a href="https://spectrum.ieee.org/ai-misinformation-llm-bullshit">IEEE Spectrum</a>
                    |
                    <a href="https://institutions.newscientist.com/article/2490861-the-way-we-train-ais-makes-them-more-likely-to-spout-bull/">New Scientist</a>
                    |
                    <a href="https://www.cnet.com/tech/services-and-software/ai-lies-to-you-because-it-thinks-thats-what-you-want/">CNET</a>
                    |
                    <a href="https://www.psychologytoday.com/us/blog/the-digital-self/202507/the-vapid-brilliance-of-artificial-intelligence">Psychology Today</a>
                    |
                    <a href="https://www.mediapost.com/publications/article/407833/the-ai-bullsht-index-and-the-psychology-behind-i.html?edition=139728">MediaPost</a>
                  </span>
                </td>
              </tr>
            </tr>

            <tr onmouseout="intro_stop()" onmouseover="intro_start()"></tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/rlhs.jpg" alt="intro" width="160" style="border-style: none">
              </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2501.08617">
                    <papertitle><b>RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation</b></papertitle>
                  </a>
                  <br>
                  <strong>Kaiqu Liang</strong>,
                  Haimin Hu,
                  Ryan Liu,
                  Thomas L. Griffiths,
                  Jaime Fernández Fisac
                  <br>
                  <em> Preprint & NeurIPS Safe Generative AI Workshop </em>

                  <br>   
                  <a href="https://arxiv.org/pdf/2501.08617">Paper</a>     
                  |
                  <a href="https://rl-hindsight.github.io/">Website</a>
                  |
                  <a href="https://github.com/SafeRoboticsLab/RLHS">Code</a>
                  |
                  <a href="images/RLHS_poster.pdf">Poster</a>
                  |
                  <a href="https://x.com/kevin_lkq/status/1879909601291928009">Tweet</a>

                  <p> We found that RLHF can induce significant misalignment when humans provide feedback while implicitly predicting future outcomes, creating incentives for LLM deception. To address this, we propose RLHS (Hindsight Simulation): By simulating future outcomes of the interaction before providing feedback, we drastically reduce misalignment. </p>
                </td>
              </tr>

            <tr onmouseout="intro_stop()" onmouseover="intro_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/teaser.png" alt="intro" width="160" style="border-style: none">
              </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2402.06529">
                    <papertitle><b>Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity</b></papertitle>
                  </a>
                  <br>
                  <strong>Kaiqu Liang</strong>,
                  Zixu Zhang,
                  Jaime Fernández Fisac
                  <br>
                  <em> Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2402.06529.pdf">Paper</a>     
                  |
                  <a href="https://introplan.github.io/">Website</a>
                  |
                  <a href="https://github.com/kevinliang888/IntroPlan">Code</a>
                  |
                  <a href="images/Introplan_poster.pdf">Poster</a>
                  |
                  <a href="https://x.com/kevin_lkq/status/1856090676129411290">Tweet</a>

                  <p>We proposed introspective planning as a systematic approach that utilizes reasoning and memory to refine the uncertainty of language agents.</p>
                </td>
              </tr>

              <tr onmouseout="rune_stop()" onmouseover="rune_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/whoplay.png" alt="intro" width="160" style="border-style: none">
                </td>
                </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2402.09246">
                      <papertitle><b>Who Plays First? Optimizing the Order of Play in Stackelberg Games with Many Robots</b></papertitle>
                    </a>
                    <br>
                    Haimin Hu,
                    Gabriele Dragotto,
                    Zixu Zhang,
                    <strong>Kaiqu Liang</strong>,
                    Bartolomeo Stellato,
                    Jaime Fernández Fisac
                    <br>
                    <em>Robotics: Science and Systems (<strong>RSS</strong>)</em>, 2024
                    <p>We introduced Branch and Play (B&P), an algorithm that effectively resolves multi-agent spatial navigation problems by determining the optimal order of play.</p>
                  </td>
                </tr>

              <tr onmouseout="rune_stop()" onmouseover="rune_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/simpleb.png" alt="intro" width="160" style="border-style: none">
                </td>
                </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2308.10402">
                      <papertitle><b>Simple Baselines for Interactive Video Retrieval with Questions and Answers</b></papertitle>
                    </a>
                    <br>
                    <strong>Kaiqu Liang</strong>,
                    Samuel Albanie
                    <br>
                    <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
                    <br>
                    <a href="https://arxiv.org/pdf/2308.10402.pdf">Paper</a>     
                    |
                    <a href="https://github.com/kevinliang888/IVR-QA-baselines">Code</a>
                    <p>We proposed several simple yet effective baselines for interactive video retrieval via question-answering.</p>
                  </td>
                </tr>

                  <tr onmouseout="rune_stop()" onmouseover="rune_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                      <img src="images/path.png" alt="intro" width="160" style="border-style: none">
                    </td>
                    </td>
                          <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2211.09961">
                          <papertitle><b>Path Independent Equilibrium Models Can Better Exploit Test-Time Computation</b></papertitle>
                        </a>
                        <br>
                        Cem Anil*,
                        Ashwini Pokle*,
                        <strong>Kaiqu Liang*</strong>,
                        Johannes Treutlein,
                        Yuhuai Wu,
                        Shaojie Bai,
                        Zico Kolter,
                        Roger Grosse
                        <br>
                        <em> Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022
                        <br>
                        <p>We demonstrated that equilibrium model improves generalization in harder instances due to their <em>path independence</em>, highlighting its importance for model performance and scalability.</p>
                      </td>
                    </tr>

                    <tr onmouseout="rune_stop()" onmouseover="rune_start()">
                      <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/acc.png" alt="intro" width="160" style="border-style: none">
                      </td>
                      </td>
                            <td style="padding:20px;width:75%;vertical-align:middle">
                          <a href="https://www.gatsby.ucl.ac.uk/~balaji/udl2021/accepted-papers/UDL2021-paper-072.pdf">
                            <papertitle><b>Out-of-Distribution Generalization with Deep Equilibrium Models</b></papertitle>
                          </a>
                          <br>
                          <strong>Kaiqu Liang*</strong>,
                          Cem Anil*,
                          Yuhuai Wu,
                          Roger Grosse
                          <br>
                          <em> <strong>ICML</strong> Workshop on Uncertainty and Robustness in Deep Learning </em>, 2021
                          <br>
                          <p>We demonstrated and discussed why Deep Equilibrium (DEQ) Models outperform fixed-depth counterparts in generalizing under distribution shifts.</p>
                        </td>
                      </tr>
          </tbody></table>

          <br>
          <table width="100%" align="middle" border="0" cellspacing="0" cellpadding="10" style="margin-bottom: 0px;">
            <tr>
              <td>
                <h2 style="margin-bottom: -18px;">Education</h2>
              </td>
            </tr>
            <tr>
              <td width="100%" valign="middle">
                <table>
                  <tbody>
                    <tr>
                      <td width="90%">
                        <b>Princeton University</b>, USA<br>
                        Ph.D. in Computer Science • Aug. 2022 to Now <br>
                      </td>
                      <td width="50%">
                        <img src="./images/princeton.png" width="75%" style="display: block; margin-left: auto; margin-right: auto;">
                      </td>
                    </tr>
                    <tr>
                      <td width="90%">
                        <b>Cambridge University</b>, UK<br>
                        MPhil in Machine Learning and Machine Intelligence • Oct. 2021 to Aug. 2022<br>
                      </td>
                      <td width="10%">
                        <img src="./images/cam.jpg" width="100%" style="display: flex; justify-content: center;">
                      </td>
                    </tr>
                    <tr>
                      <td width="90%">
                        <b>University of Toronto</b>, Canada<br>
                        Honours Bachelor of Science • Sep. 2017 to May 2021<br>
                        Computer Science Specialist & Statistics Major & Mathematics Minor <br>
                        CGPA: 3.99/4.00 (94.1%)
                      </td>
                      <td width="10%">
                        <img src="./images/uoft.png" width="100%" style="display: block; margin-left: auto; margin-right: auto;">
                      </td>
                    </tr>
                  </tbody>
                </table>
              </td>
            </tr>
          </table>

          <br>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="margin-bottom: 10px;">
            <tr>
              <td>
                <h2>Teaching</h2>
              </td>
            </tr>
            <tr>
              <td width="60%" valign="middle">
                  <div class="title">
                    Teaching Assistant &bull; ECE346/COS348/MAE346: Intelligent Robotic Systems &bull; Princeton University
                  </div>
                  <div class="title">
                    Teaching Assistant &bull; COS 350: Ethics of computing &bull; Princeton University
                  </div>
                  <div class="title">
                    Teaching Assistant &bull; CSC165: Mathematical Expression and Reasoning for Computer Science &bull; University of Toronto
                  </div>
                </td>
              </tr> 

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
              <tr>
                <td>
                  <h2>Reviewer services</h2>
                </td>
              </tr>                
                  <td width="60%" valign="middle">
                    <div class="title">
                        International Conference on Learning Representations (<strong>ICLR</strong>)
                    </div>
                      <div class="title">
                        Neural Information Processing Systems (<strong>NeurIPS</strong>)
                      </div>
                      <div class="title">
                        International Conference on Machine Learning (<strong>ICML</strong>)
                      </div>
                      <div class="title">
                        European Conference on Computer Vision (<strong>ECCV</strong>)
                      </div>
                      <div class="title">
                        Computer Vision and Pattern Recognition Conference (<strong>CVPR</strong>)
                      </div>
                    </td>
                  </tr> 
                  <tr>    
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                Website source from <a href="https://github.com/jonbarron/website">Jon Barron</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
